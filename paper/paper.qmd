---
title: "Magic Vs Evolution"
subtitle: "Or more specifically language used in Harry Potter vs Darwin's Natural Selection"
author: "Shreya Sakura Noskor"
thanks: "Code and data are available at: [HarryDarwin](https://github.com/NotSakura/HarryDarwin.git)"
date: today
date-format: long
abstract: "This paper analyses the frequency of occurrence of certain words (\"magic\" and \"natural\") in the books Harry Potter and The Prisoner of Azkaban and Charles Darwin's Book of Evolution. Using the data provided by Gutenburg's project and PDFDrives, we were able to find the related data and analyse whether a topic of the book had any effect on the number of occurences of each word in each text. The data found here will be realted to a ver important topic in Machine Learning; clusetering. This is when a machine is able to see the content of data, and group them together based on what it sees. "
format: pdf
header-includes:
  - \usepackage{float} 
  - \floatplacement{table}{H}
number-sections: true
include-in-header: 
  text:
    \renewcommand{\abstractname}{Abstract}
bibliography: references.bib
toc: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(gutenbergr)
library(pdftools)
library(arrow)
library(dplyr)
library(stringr)
library(rstanarm)
library(marginaleffects)
```


```{r}
#| echo: false
#| warning: false
#| message: false
darwin <- read_csv(
  "../data/raw_data/darwin.csv",
  col_types = cols(
    gutenberg_id = col_integer(),
    text = col_character()
  )
)

harry <- read_csv(
  "../data/raw_data/harry.csv",
  col_types = cols(
    page_num = col_integer(),
    text = col_character()
  )
)



darwin_reduced <- darwin |>
  filter(!is.na(text)) |>
  filter(!grepl("\\*", text)) |>
  mutate(
    natural_count = str_count(text, regex("natural|species", ignore_case = T)),
    magic_count = str_count(text, regex("magic|miracle|wizard", ignore_case = T)),
    word_count = str_count(text, "\\w+")
  )

write_csv(darwin_reduced, "../data/analysis_data/darwin_reduced.csv")
arrow::write_parquet(darwin_reduced, "../data/analysis_data/darwin_reduced.parquet")


harry_reduced <- harry |>
  filter(!is.na(raw_text)) |>
  filter(!grepl("\\*", raw_text)) |>
  mutate(
    natural_count = str_count(raw_text, regex("natural|species", ignore_case = T)),
    magic_count = str_count(raw_text, regex("magic|miracle|wizard", ignore_case = T)),
    word_count = str_count(raw_text, "\\w+")
  )

write_csv(darwin_reduced, "../data/analysis_data/harry_reduced.csv")
arrow::write_parquet(darwin_reduced, "../data/analysis_data/harry_reduced.parquet")
```


Training machine learning models with language. the purpose of this text is not to discover a new thing but to helpl people understand the way machine learning models use these informations to train them. 

# Introduction

Research Question: Does the topic we talk about influence our language, or does the laguage influence the theme of the topic. 


The estimand is how often the selected words appear in each of our texts. 


# Data {#sec-data}

## Source
The data utilized was from Project Gutenburg and PDF Drives and with the help of R [@citeR] we were able to create this paper. Also code for making the models were made referencing Telling Stories by Rohan Alexander[@rohan]. Other R packages were used to clean, process and model the data such as, @tidy, @gut, @rstanarm, @pdf, @dplyr, @arrow, @stringr, @marginaleffects.

## Variables

### Distribution of each word with each text 



```{r}
#| echo: false
#| warning: false
#| message: false
#| #| label: tbl-nature
#| tbl-cap: "Comparison of number of \"nature\"-realted words and the number of words in a line"

combined_data <- bind_rows(
  mutate(harry_reduced, dataset = "Harry"),
  mutate(darwin_reduced, dataset = "Darwin")
)

combined_plot <- combined_data %>%
  mutate(log_word_count = log(word_count + 1)) %>%
  ggplot(aes(x = log_word_count, y = natural_count, color = dataset)) +
  geom_jitter(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  scale_x_continuous(
    trans = "log2",
    breaks = c(0, 1, 2, 4, 8, 16, 32, 64, 128),
    labels = c(0, 1, 2, 4, 8, 16, 32, 64, 128)
  ) +
  labs(
    x = "Log(Number of words in the line)",
    y = "Number of \"magic\" in the line",
    color = "Dataset"
  )

# Display the combined plot
print(combined_plot)
```




```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-magic
#| tbl-cap: "Comparison of number of \"magic\"-realted words and the number of words in a line"



# Create the combined plot
combined_plot <- combined_data %>%
  mutate(log_word_count = log(word_count + 1)) %>%
  ggplot(aes(x = log_word_count, y = magic_count, color = dataset)) +
  geom_jitter(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  theme_minimal() +
  scale_x_continuous(
    trans = "log2",
    breaks = c(0, 1, 2, 4, 8, 16, 32, 64, 128),
    labels = c(0, 1, 2, 4, 8, 16, 32, 64, 128)
  ) +
  labs(
    x = "Log(Number of words in the line)",
    y = "Number of \"magic\" in the line",
    color = "Dataset"
  )

# Display the combined plot
print(combined_plot)
```



# Model

## Natural and the 2 texts

Define $y_i$ is the number of times "natural" appeared in each text and the explanatory variable is the number of words in the line. This means that we have 4 models in total with the $y_i$'s being, number of times the word "natural" and "species" showed up in The book of Evolution or Harry Potter and the prisoner of azkaban. Also the number of times the word "magic", "wizard" and "miracle" showed up in The book of Evolution or Harry Potter and the prisoner of azkaban. We predict to see that there is a positive correlation in Darwin's text but not in Harry Potter due to the difference in topics. 

\begin{align} 
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i) \\
\mbox{log}(\lambda_i) &= \beta_0 + \beta_1 \times \mbox{Number } \mbox{of } \mbox{Words}_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.
```{r}
#| echo: false
#| warning: false
#| message: false
darwin_nature <- readRDS("../models/darwin_nature.rds")


harry_nature <- readRDS("../models/harry_nature.rds")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-nature_pred_darwin
#| fig-cap: "Predicted number of \"nature\"-realted words in each line based on number of words in Darwin's Book "

plot_predictions(darwin_nature, condition = "word_count") +
  labs(
    x = "Number of words",
    y = "Average number of \"natural\"s in the first 10 lines"
  ) +
  theme_classic()
```
positive correlation it seems

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-nature_pred_harry
#| fig-cap: "Predicted number of \"nature\"-realted words in each line based on number of words in Harry Potter "


plot_predictions(harry_nature, condition = "word_count") +
  labs(
    x = "Number of words",
    y = "Average number of \"natural\"s in the first 10 lines"
  ) +
  theme_classic()
```
No correlation at all

### Model justification

We predict to see that there is a positive correlation in Darwin's text but not in Harry Potter due to the difference in topics.

## "Magic" and the 2 texts
Define $y_i$ is the number of times "magic" or "miracle" appeared in the text and the explanatory variable is the number of words in the line.

\begin{align} 
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i) \\
\mbox{log}(\lambda_i) &= \beta_0 + \beta_1 \times \mbox{Number } \mbox{of } \mbox{Words}_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\end{align}


We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.
```{r}
#| echo: false
#| warning: false
#| message: false

darwin_magic <- readRDS("../models/darwin_magic.rds")

harry_magic <- readRDS("../models/harry_magic.rds")
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-magic_pred_darwin
#| fig-cap: "Predicted number of \"magic\"-realted words in each line based on number of words in Darwin's Book "
plot_predictions(darwin_magic, condition = "word_count") +
  labs(
    x = "Number of words",
    y = "Average number of \"magic\"s or \"miracle\"s appearing"
  ) +
  theme_classic()
```
As we can see here there is almost no correlation at all. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-magic_pred_harry
#| fig-cap: "Predicted number of \"magic\"-realted words in each line based on number of words in Harry Potter "


plot_predictions(harry_magic, condition = "word_count") +
  labs(
    x = "Number of words",
    y = "Average number of \"magic\"s or \"miracle\"s appearing"
  ) +
  theme_classic()
```

### Model justification

We predict to see that there is a positive correlation in Harry Potter but not in Darwin's text due to the difference in topics.


# Results

Our results are summarized below.

```{r}
#| echo: false
#| eval: true
#| label: tbl-darwin_nature
#| tbl-cap: "Model Summary showcasing the correlation coeffcient for Darwin's evolution book and the word nature"
#| warning: false

modelsummary::modelsummary(
  list(
    "darwin and nature" = darwin_nature
  ),
  statistic = "mad",
  fmt = 2
)
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-darwin_magic
#| tbl-cap: "Model Summary showcasing the correlation coeffcient for Darwin's evolution book and the word magic"
#| warning: false

modelsummary::modelsummary(
  list(
    "darwin and magic" = darwin_magic
  ),
  statistic = "mad",
  fmt = 2
)
```



```{r}
#| echo: false
#| eval: true
#| label: tbl-harry_nature
#| tbl-cap: "Model Summary showcasing the correlation coeffcient for  Harry Potter and The Prisoner of Azkaban and the word nature"
#| warning: false

modelsummary::modelsummary(
  list(
    "harry and nature" = harry_nature
  ),
  statistic = "mad",
  fmt = 2
)
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-harry_magic
#| tbl-cap: "Model Summary showcasing the correlation coeffcient for Harry Potter and The Prisoner of Azkaban and the word magic"
#| warning: false

modelsummary::modelsummary(
  list(
    "harry and magic" = harry_magic
  ),
  statistic = "mad",
  fmt = 2
)
```


# Discussion

## Why is the result the way it is {#sec-first-point}


## Importance

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}



\newpage


# References
